#!/usr/bin/env python3
"""
Jeon Contrarian Trading System - 'Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå' Analysis & Trading Bot

This bot monitors the YouTube channel 'Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå', analyzes content using AI,
and implements contrarian trading strategies (betting against Jeon's predictions).

Workflow:
1. Monitor RSS feed for new videos
2. Extract and transcribe audio using OpenAI Whisper
3. Analyze content with AI to detect market sentiment
4. Generate contrarian investment recommendations
5. Execute automated trading (future integration)
"""

import os
import sys
import json
import logging
import asyncio
import yaml
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Third-party imports
import feedparser
import yt_dlp
from openai import OpenAI
from mcp_agent.agents.agent import Agent
from mcp_agent.app import MCPApp
from mcp_agent.workflows.llm.augmented_llm import RequestParams
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

# Setup directories - script is now in events/ directory
DATA_DIR = Path(".")  # Current directory (events/)
SECRETS_DIR = Path("..")  # Parent directory for config files

# Configure logging
log_file = DATA_DIR / f"jeon_contrarian_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file)
    ]
)
logger = logging.getLogger(__name__)

# Constants
CHANNEL_ID = "UCznImSIaxZR7fdLCICLdgaQ"  # Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå
RSS_URL = f"https://www.youtube.com/feeds/videos.xml?channel_id={CHANNEL_ID}"
VIDEO_HISTORY_FILE = DATA_DIR / "jeon_video_history.json"
AUDIO_FILE = DATA_DIR / "temp_audio.mp3"


class JeonContrarianTrading:
    """Main trading bot class for contrarian strategy based on Jeon's analysis"""

    def __init__(self):
        """Initialize trading bot with OpenAI client"""
        # Load API key from mcp_agent.secrets.yaml
        secrets_file = SECRETS_DIR / "mcp_agent.secrets.yaml"
        if not secrets_file.exists():
            raise FileNotFoundError(
                "mcp_agent.secrets.yaml not found. "
                "Please copy mcp_agent.secrets.yaml.example and configure your API keys."
            )

        with open(secrets_file, 'r', encoding='utf-8') as f:
            secrets = yaml.safe_load(f)

        openai_api_key = secrets.get('openai', {}).get('api_key')
        if not openai_api_key or openai_api_key == "example key":
            raise ValueError(
                "OPENAI_API_KEY not found or not configured in mcp_agent.secrets.yaml. "
                "Please set openai.api_key in the secrets file."
            )

        self.openai_client = OpenAI(api_key=openai_api_key)
        logger.info("OpenAI client initialized successfully")

    def fetch_latest_videos(self) -> List[Dict[str, str]]:
        """
        Fetch latest videos from RSS feed

        Returns:
            List of video dictionaries with id, title, published, link
        """
        logger.info(f"Fetching RSS feed from: {RSS_URL}")

        try:
            feed = feedparser.parse(RSS_URL)
            videos = []

            for entry in feed.entries:
                video = {
                    'id': entry.yt_videoid,
                    'title': entry.title,
                    'published': entry.published,
                    'link': entry.link,
                    'author': entry.author if hasattr(entry, 'author') else 'Unknown'
                }
                videos.append(video)

            logger.info(f"Found {len(videos)} videos in feed")
            return videos

        except Exception as e:
            logger.error(f"Error fetching RSS feed: {e}", exc_info=True)
            return []

    def load_previous_videos(self) -> List[Dict[str, str]]:
        """
        Load previous video list from JSON file

        Returns:
            List of previous video dictionaries
        """
        if not Path(VIDEO_HISTORY_FILE).exists():
            logger.info("No previous video history found")
            return []

        try:
            with open(VIDEO_HISTORY_FILE, 'r', encoding='utf-8') as f:
                videos = json.load(f)
            logger.info(f"Loaded {len(videos)} previous videos")
            return videos
        except Exception as e:
            logger.error(f"Error loading video history: {e}", exc_info=True)
            return []

    def save_video_history(self, videos: List[Dict[str, str]]):
        """
        Save current video list to JSON file

        Args:
            videos: List of video dictionaries
        """
        try:
            with open(VIDEO_HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(videos, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved {len(videos)} videos to history")
        except Exception as e:
            logger.error(f"Error saving video history: {e}", exc_info=True)

    def find_new_videos(self, current_videos: List[Dict], previous_videos: List[Dict]) -> List[Dict]:
        """
        Find new videos by comparing current and previous lists

        Args:
            current_videos: Current video list
            previous_videos: Previous video list

        Returns:
            List of new video dictionaries
        """
        previous_ids = {video['id'] for video in previous_videos}
        new_videos = [video for video in current_videos if video['id'] not in previous_ids]

        logger.info(f"Found {len(new_videos)} new videos")
        return new_videos

    def extract_audio(self, video_url: str) -> Optional[str]:
        """
        Extract audio from YouTube video using yt-dlp

        Args:
            video_url: YouTube video URL

        Returns:
            Path to extracted audio file, or None on failure
        """
        logger.info(f"Extracting audio from: {video_url}")

        # Remove all existing temp_audio files (including intermediates)
        for temp_file in DATA_DIR.glob('temp_audio.*'):
            try:
                temp_file.unlink()
                logger.debug(f"Removed existing file: {temp_file}")
            except Exception as e:
                logger.warning(f"Failed to remove {temp_file}: {e}")

        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': str(DATA_DIR / 'temp_audio.%(ext)s'),
            'postprocessors': [{
                'key': 'FFmpegExtractAudio',
                'preferredcodec': 'mp3',
            }],
            'keepvideo': False,  # Delete original file after conversion
            'quiet': True,
            'no_warnings': True,
        }

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([video_url])

            if AUDIO_FILE.exists():
                logger.info("Audio extraction successful")
                return str(AUDIO_FILE)
            else:
                logger.error("Audio file not found after extraction")
                return None

        except Exception as e:
            logger.error(f"Error extracting audio: {e}", exc_info=True)
            return None

    def transcribe_audio(self, audio_file: str) -> Optional[str]:
        """
        Transcribe audio using OpenAI Whisper API
        Handles files larger than 25MB by splitting into chunks

        Args:
            audio_file: Path to audio file

        Returns:
            Transcribed text, or None on failure
        """
        logger.info(f"Transcribing audio file: {audio_file}")

        try:
            # Check file size
            file_size = Path(audio_file).stat().st_size
            max_size = 25 * 1024 * 1024  # 25MB in bytes

            logger.info(f"Audio file size: {file_size / (1024*1024):.2f} MB")

            if file_size <= max_size:
                # File is small enough, transcribe directly
                logger.info("File size is within limit, transcribing directly")
                with open(audio_file, "rb") as f:
                    result = self.openai_client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        language="ko"
                    )
                transcript = result.text
                logger.info(f"Transcription successful ({len(transcript)} characters)")
                return transcript

            else:
                # File is too large, split and transcribe in chunks
                logger.info("File exceeds 25MB limit, splitting into chunks")
                return self._transcribe_large_file(audio_file)

        except Exception as e:
            logger.error(f"Error transcribing audio: {e}", exc_info=True)
            return None

    def _transcribe_large_file(self, audio_file: str) -> Optional[str]:
        """
        Split large audio file and transcribe in chunks

        Args:
            audio_file: Path to audio file

        Returns:
            Combined transcript text
        """
        try:
            from pydub import AudioSegment

            # Load audio file
            logger.info("Loading audio file for splitting")
            audio = AudioSegment.from_mp3(audio_file)
            duration_ms = len(audio)
            duration_min = duration_ms / 60000

            logger.info(f"Audio duration: {duration_min:.2f} minutes")

            # Split into 10-minute chunks (adjust as needed)
            chunk_length_ms = 10 * 60 * 1000  # 10 minutes
            chunks = []
            transcripts = []

            for i in range(0, duration_ms, chunk_length_ms):
                chunk = audio[i:i + chunk_length_ms]
                chunk_file = DATA_DIR / f"temp_audio_chunk_{i//chunk_length_ms}.mp3"
                chunk.export(chunk_file, format="mp3")
                chunks.append(chunk_file)
                logger.info(f"Created chunk {len(chunks)}: {chunk_file.name}")

            # Transcribe each chunk
            for idx, chunk_file in enumerate(chunks, 1):
                logger.info(f"Transcribing chunk {idx}/{len(chunks)}")
                try:
                    with open(chunk_file, "rb") as f:
                        result = self.openai_client.audio.transcriptions.create(
                            model="whisper-1",
                            file=f,
                            language="ko"
                        )
                    transcripts.append(result.text)
                    logger.info(f"Chunk {idx} transcribed ({len(result.text)} characters)")
                except Exception as e:
                    logger.error(f"Error transcribing chunk {idx}: {e}")
                    transcripts.append(f"[Ï≤≠ÌÅ¨ {idx} Î≥ÄÌôò Ïã§Ìå®]")

            # Combine transcripts
            full_transcript = " ".join(transcripts)
            logger.info(f"Combined transcript: {len(full_transcript)} characters from {len(chunks)} chunks")

            # Cleanup chunk files
            for chunk_file in chunks:
                try:
                    chunk_file.unlink()
                    logger.debug(f"Removed chunk file: {chunk_file}")
                except Exception as e:
                    logger.warning(f"Failed to remove chunk file {chunk_file}: {e}")

            return full_transcript

        except ImportError:
            logger.error("pydub is not installed. Please install it: pip install pydub")
            logger.error("Also ensure ffmpeg is installed for audio processing")
            return None
        except Exception as e:
            logger.error(f"Error splitting/transcribing large file: {e}", exc_info=True)
            return None

    def create_analysis_agent(self, video_info: Dict, transcript: str) -> Agent:
        """
        Create AI agent for content analysis and investment recommendation

        Args:
            video_info: Video metadata dictionary
            transcript: Transcribed text

        Returns:
            Configured Agent instance
        """
        instruction = f"""ÎãπÏã†ÏùÄ Ïú†ÌäúÎ∏å Ï±ÑÎÑê 'Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå'Ïùò ÏΩòÌÖêÏ∏†Î•º Î∂ÑÏÑùÌïòÎäî Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.

## Î∂ÑÏÑù ÎåÄÏÉÅ ÏòÅÏÉÅ
- Ï†úÎ™©: {video_info['title']}
- Í≤åÏãúÏùº: {video_info['published']}
- URL: {video_info['link']}

## ÏòÅÏÉÅ ÏûêÎßâ Ï†ÑÎ¨∏
{transcript}

## Î∂ÑÏÑù Í≥ºÏ†ú

### 1Îã®Í≥Ñ: ÏΩòÌÖêÏ∏† Ïú†Ìòï ÌåêÎ≥Ñ
Îã§ÏùåÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî:
- Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏Ïù¥ ÏßÅÏ†ë Ï∂úÏó∞ÌïòÏó¨ ÏùòÍ≤¨ÏùÑ Ï†úÏãúÌïòÎäî ÏòÅÏÉÅÏù∏Í∞Ä?
- Îã®Ïàú Îâ¥Ïä§ ÏöîÏïΩÏù¥ÎÇò Í≤åÏä§Ìä∏ Ïù∏ÌÑ∞Î∑∞Îßå ÏûàÎäî ÏòÅÏÉÅÏùÄ ÏïÑÎãåÍ∞Ä?

**ÌåêÎ≥Ñ Í≤∞Í≥º**: "Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏ ÏùòÍ≤¨" ÎòêÎäî "Ïä§ÌÇµ ÎåÄÏÉÅ" Ï§ë ÌïòÎÇòÎ°ú Î™ÖÏãú

### 2Îã®Í≥Ñ: ÏãúÏû• Ï†ÑÎßù Î∂ÑÏÑù (Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏ ÏùòÍ≤¨Ïù∏ Í≤ΩÏö∞Îßå)
Ï†ÑÏù∏Íµ¨Í∞Ä ÏãúÏû•Ïóê ÎåÄÌï¥ Ïñ¥Îñ§ Í∏∞Ï°∞Î°ú ÎßêÌïòÍ≥† ÏûàÎäîÏßÄ Î∂ÑÏÑù:
- **ÏÉÅÏäπ Í∏∞Ï°∞**: ÎÇôÍ¥ÄÏ†Å Ï†ÑÎßù, Îß§Ïàò Ï∂îÏ≤ú, Í∏çÏ†ïÏ†Å ÏãúÍ∑∏ÎÑê Í∞ïÏ°∞
- **ÌïòÎùΩ Í∏∞Ï°∞**: ÎπÑÍ¥ÄÏ†Å Ï†ÑÎßù, Îß§ÎèÑ/Í¥ÄÎßù Ï∂îÏ≤ú, Î∂ÄÏ†ïÏ†Å ÏãúÍ∑∏ÎÑê Í∞ïÏ°∞
- **Ï§ëÎ¶Ω Í∏∞Ï°∞**: Î™ÖÌôïÌïú Î∞©Ìñ•ÏÑ± ÏóÜÏùå

**ÏãúÏû• Í∏∞Ï°∞ ÌåêÎã®**: ÏÉÅÏäπ/ÌïòÎùΩ/Ï§ëÎ¶Ω Ï§ë ÌïòÎÇòÎ°ú Î™ÖÏãú
**Í∑ºÍ±∞**: ÏûêÎßâÏóêÏÑú Ìï¥Îãπ ÌåêÎã®ÏùÑ ÎÇ¥Î¶∞ ÌïµÏã¨ Î∞úÏñ∏ Ïù∏Ïö© (3-5Í∞ú)

### 3Îã®Í≥Ñ: ÏΩòÌÖêÏ∏† ÏöîÏïΩ
ÏòÅÏÉÅÏùò ÌïµÏã¨ ÎÇ¥Ïö©ÏùÑ 3-5Í∞ú Î∂àÎ¶ø Ìè¨Ïù∏Ìä∏Î°ú ÏöîÏïΩ
- Ï£ºÏöî ÎÖºÏ†ê
- Ïñ∏Í∏âÎêú Í≤ΩÏ†ú ÏßÄÌëúÎÇò Ïù¥Ïäà
- Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Ïñ∏Í∏âÎêú Ï¢ÖÎ™©/ÏÑπÌÑ∞ (ÏûàÎäî Í≤ΩÏö∞)

### 4Îã®Í≥Ñ: Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎûµ (Contrarian Investment)
Ï†ÑÏù∏Íµ¨Ïùò ÏùòÍ≤¨Í≥º **Î∞òÎåÄ** Î∞©Ìñ•ÏúºÎ°ú Î≤†ÌåÖÌïòÎäî Ï†ÑÎûµ Ï†úÏãú:

**ÎßåÏïΩ ÏÉÅÏäπ Í∏∞Ï°∞ÎùºÎ©¥ (ÌïòÎùΩÏóê Î≤†ÌåÖ)**:
- Ïù∏Î≤ÑÏä§(Inverse) ETF/ETN Ï∂îÏ≤ú
  - KODEX Ïù∏Î≤ÑÏä§ (114800)
  - TIGER Ïù∏Î≤ÑÏä§ (252670)
  - KODEX ÏΩîÏä§Îã•150 Ïù∏Î≤ÑÏä§ (251340)
- Î∞©Ïñ¥Ï£º Ï∂îÏ≤ú (Ìó¨Ïä§ÏºÄÏñ¥, ÌïÑÏàòÏÜåÎπÑÏû¨ Îì±)
- ÌíãÏòµÏÖò Ï†ÑÎûµ Í∞ÄÎä• Ï¢ÖÎ™©

**ÎßåÏïΩ ÌïòÎùΩ Í∏∞Ï°∞ÎùºÎ©¥ (ÏÉÅÏäπÏóê Î≤†ÌåÖ)**:
- Î†àÎ≤ÑÎ¶¨ÏßÄ(Leverage) ETF/ETN Ï∂îÏ≤ú
  - KODEX Î†àÎ≤ÑÎ¶¨ÏßÄ (122630)
  - TIGER Î†àÎ≤ÑÎ¶¨ÏßÄ (233740)
  - KODEX ÏΩîÏä§Îã•150 Î†àÎ≤ÑÎ¶¨ÏßÄ (233160)
- ÏÑ±Ïû•Ï£º/Î™®Î©òÌÖÄÏ£º Ï∂îÏ≤ú
- ÏΩúÏòµÏÖò Ï†ÑÎûµ Í∞ÄÎä• Ï¢ÖÎ™©

**ÎßåÏïΩ Ï§ëÎ¶Ω Í∏∞Ï°∞ÎùºÎ©¥**:
- Í¥ÄÎßù Ï∂îÏ≤ú
- Î≥ÄÎèôÏÑ± Í¥ÄÎ†® ÏÉÅÌíà Í≤ÄÌÜ†

### 5Îã®Í≥Ñ: Î¶¨Ïä§ÌÅ¨ Í≤ΩÍ≥†
Ïó≠Î∞úÏÉÅ Ï†ÑÎûµÏùò Î¶¨Ïä§ÌÅ¨ Î™ÖÏãú:
- Ï†ÑÏù∏Íµ¨Ïùò ÏùòÍ≤¨Ïù¥ ÎßûÏùÑ Í≤ΩÏö∞Ïùò ÏÜêÏã§ ÏãúÎÇòÎ¶¨Ïò§
- Í∂åÏû• ÏÜêÏ†àÎß§ ÎπÑÏú® (Ïòà: -5%, -10%)
- Ìè¨ÏßÄÏÖò ÏÇ¨Ïù¥Ïßï Í∂åÏû• (Ï†ÑÏ≤¥ ÏûêÏÇ∞Ïùò Î™á %Î°ú Ï†úÌïú)

## Ï∂úÎ†• ÌòïÏãù
Îã§Ïùå ÌòïÏãùÏúºÎ°ú Íµ¨Ï°∞ÌôîÎêú Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï∂úÎ†•ÌïòÏÑ∏Ïöî:

```
# Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Î∂ÑÏÑù

## üì∫ ÏòÅÏÉÅ Ï†ïÎ≥¥
- **Ï†úÎ™©**: {video_info['title']}
- **Í≤åÏãúÏùº**: {video_info['published']}
- **URL**: {video_info['link']}

## 1Ô∏è‚É£ ÏΩòÌÖêÏ∏† Ïú†Ìòï ÌåêÎ≥Ñ
[Ï†ÑÏù∏Íµ¨ Î≥∏Ïù∏ ÏùòÍ≤¨ / Ïä§ÌÇµ ÎåÄÏÉÅ]

## 2Ô∏è‚É£ ÏãúÏû• Í∏∞Ï°∞ Î∂ÑÏÑù
**ÌåêÎã®**: [ÏÉÅÏäπ/ÌïòÎùΩ/Ï§ëÎ¶Ω]

**Í∑ºÍ±∞**:
- [Ïù∏Ïö©1]
- [Ïù∏Ïö©2]
- [Ïù∏Ïö©3]

## 3Ô∏è‚É£ ÏòÅÏÉÅ ÎÇ¥Ïö© ÏöîÏïΩ
- ÌïµÏã¨ ÎÖºÏ†ê 1
- ÌïµÏã¨ ÎÖºÏ†ê 2
- ÌïµÏã¨ ÎÖºÏ†ê 3

## 4Ô∏è‚É£ Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎûµ
### Ï∂îÏ≤ú Ìè¨ÏßÄÏÖò: [Îß§Ïàò/Îß§ÎèÑ/Í¥ÄÎßù]

### Ï∂îÏ≤ú Ï¢ÖÎ™©/ÏÉÅÌíà
1. **[Ï¢ÖÎ™©Î™Ö] (Ï¢ÖÎ™©ÏΩîÎìú)**
   - Ïú†Ìòï: [ETF/ETN/Í∞úÎ≥ÑÏ£º]
   - Ïù¥Ïú†: ...

2. **[Ï¢ÖÎ™©Î™Ö] (Ï¢ÖÎ™©ÏΩîÎìú)**
   - Ïú†Ìòï: [ETF/ETN/Í∞úÎ≥ÑÏ£º]
   - Ïù¥Ïú†: ...

### ÏßÑÏûÖ Ï†ÑÎûµ
- ÌÉÄÏù¥Î∞ç: ...
- Î∂ÑÌï†Îß§Ïàò Í∂åÏû•: ...

## 5Ô∏è‚É£ Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨
- ‚ö†Ô∏è ÏÜêÏ†àÎß§: -X% ÎèÑÎã¨ Ïãú Î¨¥Ï°∞Í±¥ Ï≤≠ÏÇ∞
- ‚ö†Ô∏è Ìè¨ÏßÄÏÖò ÌÅ¨Í∏∞: Ï†ÑÏ≤¥ ÏûêÏÇ∞Ïùò Y% Ïù¥ÌïòÎ°ú Ï†úÌïú
- ‚ö†Ô∏è Ï†ÑÏù∏Íµ¨ ÏùòÍ≤¨Ïù¥ ÎßûÏùÑ Í≤ΩÏö∞ ÏòàÏÉÅ ÏÜêÏã§: ...
```

## Ï£ºÏùòÏÇ¨Ìï≠
- ÏûêÎßâ ÎÇ¥Ïö©ÎßåÏùÑ Í∑ºÍ±∞Î°ú Î∂ÑÏÑùÌïòÏÑ∏Ïöî (Ï∂îÏ∏° Í∏àÏßÄ)
- Ï†ÑÏù∏Íµ¨Í∞Ä ÏßÅÏ†ë Ïñ∏Í∏âÌïòÏßÄ ÏïäÏùÄ Ï¢ÖÎ™©ÏùÄ Ïã†Ï§ëÌïòÍ≤å Ï∂îÏ≤úÌïòÏÑ∏Ïöî
- Ïó≠Î∞úÏÉÅ Ï†ÑÎûµÏùò ÎÜíÏùÄ Î¶¨Ïä§ÌÅ¨Î•º Î™ÖÌôïÌûà Í≤ΩÍ≥†ÌïòÏÑ∏Ïöî
- Ìà¨Ïûê Í∂åÏú†Í∞Ä ÏïÑÎãå Ï†ïÎ≥¥ Ï†úÍ≥µ Î™©Ï†ÅÏûÑÏùÑ Î™ÖÏãúÌïòÏÑ∏Ïöî
"""

        return Agent(
            name="youtube_event_fund_analyst",
            instruction=instruction,
            server_names=[]  # No MCP servers needed for transcript analysis
        )

    async def analyze_video(self, video_info: Dict, transcript: str) -> str:
        """
        Analyze video content using AI agent

        Args:
            video_info: Video metadata
            transcript: Transcribed text

        Returns:
            Analysis result text
        """
        logger.info(f"Analyzing video: {video_info['title']}")

        try:
            agent = self.create_analysis_agent(video_info, transcript)

            # Initialize MCPApp context
            app = MCPApp(name="jeon_contrarian_trading_analysis")

            async with app.run() as _:
                # Attach LLM to agent within MCPApp context
                llm = await agent.attach_llm(OpenAIAugmentedLLM)

                # Generate analysis using the agent
                result = await llm.generate_str(
                    message="ÏúÑ ÏßÄÏãúÏÇ¨Ìï≠Ïóê Îî∞Îùº ÏòÅÏÉÅÏùÑ Î∂ÑÏÑùÌïòÍ≥† Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Ï†ÑÎûµÏùÑ Ï†úÏãúÌï¥Ï£ºÏÑ∏Ïöî.",
                    request_params=RequestParams(
                        model="gpt-5",
                        maxTokens=16000,
                        max_iterations=3,
                        parallel_tool_calls=False,
                        use_history=True
                    )
                )

            logger.info("Analysis completed successfully")
            return result

        except Exception as e:
            logger.error(f"Error during analysis: {e}", exc_info=True)
            return f"Î∂ÑÏÑù Ïã§Ìå®: {str(e)}"

    def cleanup_temp_files(self):
        """Remove temporary audio files including chunks"""
        # Remove all temp_audio files (including intermediates and chunks)
        cleaned_files = []

        # Clean up main audio files
        for temp_file in DATA_DIR.glob('temp_audio.*'):
            try:
                temp_file.unlink()
                cleaned_files.append(temp_file.name)
            except Exception as e:
                logger.warning(f"Failed to clean up {temp_file}: {e}")

        # Clean up chunk files
        for chunk_file in DATA_DIR.glob('temp_audio_chunk_*.mp3'):
            try:
                chunk_file.unlink()
                cleaned_files.append(chunk_file.name)
            except Exception as e:
                logger.warning(f"Failed to clean up {chunk_file}: {e}")

        if cleaned_files:
            logger.info(f"Cleaned up temporary audio files: {', '.join(cleaned_files)}")
        else:
            logger.debug("No temporary audio files to clean up")

    async def process_new_video(self, video_info: Dict) -> Optional[str]:
        """
        Process a new video: extract audio, transcribe, analyze

        Args:
            video_info: Video metadata dictionary

        Returns:
            Analysis result text, or None on failure
        """
        logger.info(f"Processing new video: {video_info['title']}")

        try:
            # Step 1: Extract audio
            audio_file = self.extract_audio(video_info['link'])
            if not audio_file:
                return None

            # Step 2: Transcribe audio
            transcript = self.transcribe_audio(audio_file)
            if not transcript:
                return None

            # Save transcript for debugging
            transcript_file = DATA_DIR / f"transcript_{video_info['id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(transcript_file, 'w', encoding='utf-8') as f:
                f.write(f"Video: {video_info['title']}\n")
                f.write(f"URL: {video_info['link']}\n")
                f.write(f"Published: {video_info['published']}\n")
                f.write(f"\n{'='*80}\n\n")
                f.write(transcript)
            logger.info(f"Transcript saved to: {transcript_file}")

            # Step 3: Analyze content
            analysis = await self.analyze_video(video_info, transcript)

            # Save analysis result
            analysis_file = DATA_DIR / f"analysis_{video_info['id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                f.write(analysis)
            logger.info(f"Analysis saved to: {analysis_file}")

            return analysis

        except Exception as e:
            logger.error(f"Error processing video: {e}", exc_info=True)
            return None

        finally:
            # Always cleanup temporary files
            self.cleanup_temp_files()

    async def process_single_video_url(self, video_url: str):
        """
        Process a single video URL directly (for testing)

        Args:
            video_url: YouTube video URL
        """
        logger.info("="*80)
        logger.info("Jeon Contrarian Trading - Single Video Mode")
        logger.info("="*80)

        try:
            # Create video info from URL
            video_info = {
                'title': 'Manual Video Input',
                'published': datetime.now().isoformat(),
                'link': video_url,
                'id': video_url.split('=')[-1] if '=' in video_url else video_url.split('/')[-1]
            }

            logger.info(f"Processing video: {video_url}")

            analysis = await self.process_new_video(video_info)

            if analysis:
                # Print analysis to console
                print("\n" + "="*80)
                print("ANALYSIS RESULT")
                print("="*80)
                print(analysis)
                print("="*80 + "\n")
            else:
                logger.warning("Failed to analyze video")

            logger.info("="*80)
            logger.info("Jeon Contrarian Trading - Completed")
            logger.info("="*80)

        except Exception as e:
            logger.error(f"Fatal error processing video: {e}", exc_info=True)
            raise

    async def run(self):
        """Main execution workflow"""
        logger.info("="*80)
        logger.info("Jeon Contrarian Trading - Starting")
        logger.info("="*80)

        try:
            # Step 1: Fetch latest videos from RSS
            current_videos = self.fetch_latest_videos()
            if not current_videos:
                logger.warning("No videos found in RSS feed")
                return

            # Step 2: Load previous video history
            previous_videos = self.load_previous_videos()

            # Check if this is first run
            is_first_run = len(previous_videos) == 0

            if is_first_run:
                logger.info("üé¨ First run detected - initializing video history")
                logger.info(f"Found {len(current_videos)} videos in channel")
                logger.info("Saving video history without processing...")

                # Save current videos and exit
                self.save_video_history(current_videos)

                logger.info("="*80)
                logger.info("‚úÖ Video history initialized successfully")
                logger.info("üí° Run again to detect and process new videos")
                logger.info("="*80)
                return

            # Step 3: Find new videos
            new_videos = self.find_new_videos(current_videos, previous_videos)

            if not new_videos:
                logger.info("No new videos found")
                return

            # Step 4: Process each new video
            for video in new_videos:
                logger.info("\n" + "="*80)
                logger.info(f"Processing: {video['title']}")
                logger.info("="*80)

                analysis = await self.process_new_video(video)

                if analysis:
                    # Print analysis to console
                    print("\n" + "="*80)
                    print("ANALYSIS RESULT")
                    print("="*80)
                    print(analysis)
                    print("="*80 + "\n")
                else:
                    logger.warning(f"Failed to analyze video: {video['title']}")

            # Step 5: Save updated video history
            self.save_video_history(current_videos)

            logger.info("="*80)
            logger.info("Jeon Contrarian Trading - Completed")
            logger.info("="*80)

        except Exception as e:
            logger.error(f"Fatal error in main workflow: {e}", exc_info=True)
            raise


async def main():
    """Entry point"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Jeon Contrarian Trading - Ï†ÑÏù∏Íµ¨Í≤ΩÏ†úÏó∞Íµ¨ÏÜå Ïó≠Î∞úÏÉÅ Ìà¨Ïûê Î∂ÑÏÑù",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Normal mode (monitor RSS feed for new videos)
  python events/jeon_contrarian_trading.py

  # Test mode (process specific video URL)
  python events/jeon_contrarian_trading.py --video-url "https://www.youtube.com/watch?v=VIDEO_ID"
        """
    )
    parser.add_argument(
        '--video-url',
        type=str,
        help='Process a specific YouTube video URL (test mode)'
    )

    args = parser.parse_args()

    try:
        crawler = YouTubeEventFundCrawler()

        if args.video_url:
            # Single video mode
            logger.info(f"üéØ Test mode: Processing single video")
            await crawler.process_single_video_url(args.video_url)
        else:
            # Normal RSS monitoring mode
            await crawler.run()

    except KeyboardInterrupt:
        logger.info("Interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
