#!/usr/bin/env python3
"""
YouTube Event Fund Crawler - 'ì „ì¸êµ¬ê²½ì œì—°êµ¬ì†Œ' Analysis System

This script monitors the YouTube channel 'ì „ì¸êµ¬ê²½ì œì—°êµ¬ì†Œ' for new videos,
transcribes them using OpenAI Whisper API, analyzes the content, and provides
contrarian investment recommendations.

Workflow:
1. Fetch latest videos from RSS feed
2. Compare with previous video list (stored in JSON)
3. Extract audio and transcribe with Whisper API
4. Analyze content and generate contrarian investment recommendations
5. Log results (future: integrate with automated trading)
"""

import os
import sys
import json
import logging
import asyncio
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Third-party imports
import feedparser
import yt_dlp
from openai import OpenAI
from mcp_agent.agents.agent import Agent
from mcp_agent.app import MCPApp
from mcp_agent.workflows.llm.augmented_llm import RequestParams
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

# Setup directories
EVENTS_DIR = Path("events")
EVENTS_DIR.mkdir(exist_ok=True)

# Configure logging
log_file = EVENTS_DIR / f"youtube_crawler_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file)
    ]
)
logger = logging.getLogger(__name__)

# Constants
CHANNEL_ID = "UCznImSIaxZR7fdLCICLdgaQ"  # ì „ì¸êµ¬ê²½ì œì—°êµ¬ì†Œ
RSS_URL = f"https://www.youtube.com/feeds/videos.xml?channel_id={CHANNEL_ID}"
VIDEO_HISTORY_FILE = EVENTS_DIR / "youtube_video_history.json"
AUDIO_FILE = EVENTS_DIR / "temp_audio.mp3"


class YouTubeEventFundCrawler:
    """Main crawler class for YouTube event fund analysis"""

    def __init__(self):
        """Initialize crawler with OpenAI client"""
        # Load API key from mcp_agent.secrets.yaml
        secrets_file = Path("mcp_agent.secrets.yaml")
        if not secrets_file.exists():
            raise FileNotFoundError(
                "mcp_agent.secrets.yaml not found. "
                "Please copy mcp_agent.secrets.yaml.example and configure your API keys."
            )

        with open(secrets_file, 'r', encoding='utf-8') as f:
            secrets = yaml.safe_load(f)

        openai_api_key = secrets.get('openai', {}).get('api_key')
        if not openai_api_key or openai_api_key == "example key":
            raise ValueError(
                "OPENAI_API_KEY not found or not configured in mcp_agent.secrets.yaml. "
                "Please set openai.api_key in the secrets file."
            )

        self.openai_client = OpenAI(api_key=openai_api_key)
        logger.info("OpenAI client initialized successfully")

    def fetch_latest_videos(self) -> List[Dict[str, str]]:
        """
        Fetch latest videos from RSS feed

        Returns:
            List of video dictionaries with id, title, published, link
        """
        logger.info(f"Fetching RSS feed from: {RSS_URL}")

        try:
            feed = feedparser.parse(RSS_URL)
            videos = []

            for entry in feed.entries:
                video = {
                    'id': entry.yt_videoid,
                    'title': entry.title,
                    'published': entry.published,
                    'link': entry.link,
                    'author': entry.author if hasattr(entry, 'author') else 'Unknown'
                }
                videos.append(video)

            logger.info(f"Found {len(videos)} videos in feed")
            return videos

        except Exception as e:
            logger.error(f"Error fetching RSS feed: {e}", exc_info=True)
            return []

    def load_previous_videos(self) -> List[Dict[str, str]]:
        """
        Load previous video list from JSON file

        Returns:
            List of previous video dictionaries
        """
        if not Path(VIDEO_HISTORY_FILE).exists():
            logger.info("No previous video history found")
            return []

        try:
            with open(VIDEO_HISTORY_FILE, 'r', encoding='utf-8') as f:
                videos = json.load(f)
            logger.info(f"Loaded {len(videos)} previous videos")
            return videos
        except Exception as e:
            logger.error(f"Error loading video history: {e}", exc_info=True)
            return []

    def save_video_history(self, videos: List[Dict[str, str]]):
        """
        Save current video list to JSON file

        Args:
            videos: List of video dictionaries
        """
        try:
            with open(VIDEO_HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(videos, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved {len(videos)} videos to history")
        except Exception as e:
            logger.error(f"Error saving video history: {e}", exc_info=True)

    def find_new_videos(self, current_videos: List[Dict], previous_videos: List[Dict]) -> List[Dict]:
        """
        Find new videos by comparing current and previous lists

        Args:
            current_videos: Current video list
            previous_videos: Previous video list

        Returns:
            List of new video dictionaries
        """
        previous_ids = {video['id'] for video in previous_videos}
        new_videos = [video for video in current_videos if video['id'] not in previous_ids]

        logger.info(f"Found {len(new_videos)} new videos")
        return new_videos

    def extract_audio(self, video_url: str) -> Optional[str]:
        """
        Extract audio from YouTube video using yt-dlp

        Args:
            video_url: YouTube video URL

        Returns:
            Path to extracted audio file, or None on failure
        """
        logger.info(f"Extracting audio from: {video_url}")

        # Remove existing audio file if present
        if AUDIO_FILE.exists():
            AUDIO_FILE.unlink()

        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': str(EVENTS_DIR / 'temp_audio.%(ext)s'),
            'postprocessors': [{
                'key': 'FFmpegExtractAudio',
                'preferredcodec': 'mp3',
            }],
            'quiet': True,
            'no_warnings': True,
        }

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([video_url])

            if AUDIO_FILE.exists():
                logger.info("Audio extraction successful")
                return str(AUDIO_FILE)
            else:
                logger.error("Audio file not found after extraction")
                return None

        except Exception as e:
            logger.error(f"Error extracting audio: {e}", exc_info=True)
            return None

    def transcribe_audio(self, audio_file: str) -> Optional[str]:
        """
        Transcribe audio using OpenAI Whisper API

        Args:
            audio_file: Path to audio file

        Returns:
            Transcribed text, or None on failure
        """
        logger.info(f"Transcribing audio file: {audio_file}")

        try:
            with open(audio_file, "rb") as f:
                result = self.openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    language="ko"
                )

            transcript = result.text
            logger.info(f"Transcription successful ({len(transcript)} characters)")
            return transcript

        except Exception as e:
            logger.error(f"Error transcribing audio: {e}", exc_info=True)
            return None

    def create_analysis_agent(self, video_info: Dict, transcript: str) -> Agent:
        """
        Create AI agent for content analysis and investment recommendation

        Args:
            video_info: Video metadata dictionary
            transcript: Transcribed text

        Returns:
            Configured Agent instance
        """
        instruction = f"""ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì±„ë„ 'ì „ì¸êµ¬ê²½ì œì—°êµ¬ì†Œ'ì˜ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ëŠ” ì—­ë°œìƒ íˆ¬ìž ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

## ë¶„ì„ ëŒ€ìƒ ì˜ìƒ
- ì œëª©: {video_info['title']}
- ê²Œì‹œì¼: {video_info['published']}
- URL: {video_info['link']}

## ì˜ìƒ ìžë§‰ ì „ë¬¸
{transcript}

## ë¶„ì„ ê³¼ì œ

### 1ë‹¨ê³„: ì½˜í…ì¸  ìœ í˜• íŒë³„
ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
- ì „ì¸êµ¬ ë³¸ì¸ì´ ì§ì ‘ ì¶œì—°í•˜ì—¬ ì˜ê²¬ì„ ì œì‹œí•˜ëŠ” ì˜ìƒì¸ê°€?
- ë‹¨ìˆœ ë‰´ìŠ¤ ìš”ì•½ì´ë‚˜ ê²ŒìŠ¤íŠ¸ ì¸í„°ë·°ë§Œ ìžˆëŠ” ì˜ìƒì€ ì•„ë‹Œê°€?

**íŒë³„ ê²°ê³¼**: "ì „ì¸êµ¬ ë³¸ì¸ ì˜ê²¬" ë˜ëŠ” "ìŠ¤í‚µ ëŒ€ìƒ" ì¤‘ í•˜ë‚˜ë¡œ ëª…ì‹œ

### 2ë‹¨ê³„: ì‹œìž¥ ì „ë§ ë¶„ì„ (ì „ì¸êµ¬ ë³¸ì¸ ì˜ê²¬ì¸ ê²½ìš°ë§Œ)
ì „ì¸êµ¬ê°€ ì‹œìž¥ì— ëŒ€í•´ ì–´ë–¤ ê¸°ì¡°ë¡œ ë§í•˜ê³  ìžˆëŠ”ì§€ ë¶„ì„:
- **ìƒìŠ¹ ê¸°ì¡°**: ë‚™ê´€ì  ì „ë§, ë§¤ìˆ˜ ì¶”ì²œ, ê¸ì •ì  ì‹œê·¸ë„ ê°•ì¡°
- **í•˜ë½ ê¸°ì¡°**: ë¹„ê´€ì  ì „ë§, ë§¤ë„/ê´€ë§ ì¶”ì²œ, ë¶€ì •ì  ì‹œê·¸ë„ ê°•ì¡°
- **ì¤‘ë¦½ ê¸°ì¡°**: ëª…í™•í•œ ë°©í–¥ì„± ì—†ìŒ

**ì‹œìž¥ ê¸°ì¡° íŒë‹¨**: ìƒìŠ¹/í•˜ë½/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë¡œ ëª…ì‹œ
**ê·¼ê±°**: ìžë§‰ì—ì„œ í•´ë‹¹ íŒë‹¨ì„ ë‚´ë¦° í•µì‹¬ ë°œì–¸ ì¸ìš© (3-5ê°œ)

### 3ë‹¨ê³„: ì½˜í…ì¸  ìš”ì•½
ì˜ìƒì˜ í•µì‹¬ ë‚´ìš©ì„ 3-5ê°œ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ìš”ì•½
- ì£¼ìš” ë…¼ì 
- ì–¸ê¸‰ëœ ê²½ì œ ì§€í‘œë‚˜ ì´ìŠˆ
- êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰ëœ ì¢…ëª©/ì„¹í„° (ìžˆëŠ” ê²½ìš°)

### 4ë‹¨ê³„: ì—­ë°œìƒ íˆ¬ìž ì „ëžµ (Contrarian Investment)
ì „ì¸êµ¬ì˜ ì˜ê²¬ê³¼ **ë°˜ëŒ€** ë°©í–¥ìœ¼ë¡œ ë² íŒ…í•˜ëŠ” ì „ëžµ ì œì‹œ:

**ë§Œì•½ ìƒìŠ¹ ê¸°ì¡°ë¼ë©´ (í•˜ë½ì— ë² íŒ…)**:
- ì¸ë²„ìŠ¤(Inverse) ETF/ETN ì¶”ì²œ
  - KODEX ì¸ë²„ìŠ¤ (114800)
  - TIGER ì¸ë²„ìŠ¤ (252670)
  - KODEX ì½”ìŠ¤ë‹¥150 ì¸ë²„ìŠ¤ (251340)
- ë°©ì–´ì£¼ ì¶”ì²œ (í—¬ìŠ¤ì¼€ì–´, í•„ìˆ˜ì†Œë¹„ìž¬ ë“±)
- í’‹ì˜µì…˜ ì „ëžµ ê°€ëŠ¥ ì¢…ëª©

**ë§Œì•½ í•˜ë½ ê¸°ì¡°ë¼ë©´ (ìƒìŠ¹ì— ë² íŒ…)**:
- ë ˆë²„ë¦¬ì§€(Leverage) ETF/ETN ì¶”ì²œ
  - KODEX ë ˆë²„ë¦¬ì§€ (122630)
  - TIGER ë ˆë²„ë¦¬ì§€ (233740)
  - KODEX ì½”ìŠ¤ë‹¥150 ë ˆë²„ë¦¬ì§€ (233160)
- ì„±ìž¥ì£¼/ëª¨ë©˜í…€ì£¼ ì¶”ì²œ
- ì½œì˜µì…˜ ì „ëžµ ê°€ëŠ¥ ì¢…ëª©

**ë§Œì•½ ì¤‘ë¦½ ê¸°ì¡°ë¼ë©´**:
- ê´€ë§ ì¶”ì²œ
- ë³€ë™ì„± ê´€ë ¨ ìƒí’ˆ ê²€í† 

### 5ë‹¨ê³„: ë¦¬ìŠ¤í¬ ê²½ê³ 
ì—­ë°œìƒ ì „ëžµì˜ ë¦¬ìŠ¤í¬ ëª…ì‹œ:
- ì „ì¸êµ¬ì˜ ì˜ê²¬ì´ ë§žì„ ê²½ìš°ì˜ ì†ì‹¤ ì‹œë‚˜ë¦¬ì˜¤
- ê¶Œìž¥ ì†ì ˆë§¤ ë¹„ìœ¨ (ì˜ˆ: -5%, -10%)
- í¬ì§€ì…˜ ì‚¬ì´ì§• ê¶Œìž¥ (ì „ì²´ ìžì‚°ì˜ ëª‡ %ë¡œ ì œí•œ)

## ì¶œë ¥ í˜•ì‹
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ì„¸ìš”:

```
# ì „ì¸êµ¬ê²½ì œì—°êµ¬ì†Œ ì—­ë°œìƒ íˆ¬ìž ë¶„ì„

## ðŸ“º ì˜ìƒ ì •ë³´
- **ì œëª©**: {video_info['title']}
- **ê²Œì‹œì¼**: {video_info['published']}
- **URL**: {video_info['link']}

## 1ï¸âƒ£ ì½˜í…ì¸  ìœ í˜• íŒë³„
[ì „ì¸êµ¬ ë³¸ì¸ ì˜ê²¬ / ìŠ¤í‚µ ëŒ€ìƒ]

## 2ï¸âƒ£ ì‹œìž¥ ê¸°ì¡° ë¶„ì„
**íŒë‹¨**: [ìƒìŠ¹/í•˜ë½/ì¤‘ë¦½]

**ê·¼ê±°**:
- [ì¸ìš©1]
- [ì¸ìš©2]
- [ì¸ìš©3]

## 3ï¸âƒ£ ì˜ìƒ ë‚´ìš© ìš”ì•½
- í•µì‹¬ ë…¼ì  1
- í•µì‹¬ ë…¼ì  2
- í•µì‹¬ ë…¼ì  3

## 4ï¸âƒ£ ì—­ë°œìƒ íˆ¬ìž ì „ëžµ
### ì¶”ì²œ í¬ì§€ì…˜: [ë§¤ìˆ˜/ë§¤ë„/ê´€ë§]

### ì¶”ì²œ ì¢…ëª©/ìƒí’ˆ
1. **[ì¢…ëª©ëª…] (ì¢…ëª©ì½”ë“œ)**
   - ìœ í˜•: [ETF/ETN/ê°œë³„ì£¼]
   - ì´ìœ : ...

2. **[ì¢…ëª©ëª…] (ì¢…ëª©ì½”ë“œ)**
   - ìœ í˜•: [ETF/ETN/ê°œë³„ì£¼]
   - ì´ìœ : ...

### ì§„ìž… ì „ëžµ
- íƒ€ì´ë°: ...
- ë¶„í• ë§¤ìˆ˜ ê¶Œìž¥: ...

## 5ï¸âƒ£ ë¦¬ìŠ¤í¬ ê´€ë¦¬
- âš ï¸ ì†ì ˆë§¤: -X% ë„ë‹¬ ì‹œ ë¬´ì¡°ê±´ ì²­ì‚°
- âš ï¸ í¬ì§€ì…˜ í¬ê¸°: ì „ì²´ ìžì‚°ì˜ Y% ì´í•˜ë¡œ ì œí•œ
- âš ï¸ ì „ì¸êµ¬ ì˜ê²¬ì´ ë§žì„ ê²½ìš° ì˜ˆìƒ ì†ì‹¤: ...
```

## ì£¼ì˜ì‚¬í•­
- ìžë§‰ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë¶„ì„í•˜ì„¸ìš” (ì¶”ì¸¡ ê¸ˆì§€)
- ì „ì¸êµ¬ê°€ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ì•Šì€ ì¢…ëª©ì€ ì‹ ì¤‘í•˜ê²Œ ì¶”ì²œí•˜ì„¸ìš”
- ì—­ë°œìƒ ì „ëžµì˜ ë†’ì€ ë¦¬ìŠ¤í¬ë¥¼ ëª…í™•ížˆ ê²½ê³ í•˜ì„¸ìš”
- íˆ¬ìž ê¶Œìœ ê°€ ì•„ë‹Œ ì •ë³´ ì œê³µ ëª©ì ìž„ì„ ëª…ì‹œí•˜ì„¸ìš”
"""

        return Agent(
            name="youtube_event_fund_analyst",
            instruction=instruction,
            server_names=[]  # No MCP servers needed for transcript analysis
        )

    async def analyze_video(self, video_info: Dict, transcript: str) -> str:
        """
        Analyze video content using AI agent

        Args:
            video_info: Video metadata
            transcript: Transcribed text

        Returns:
            Analysis result text
        """
        logger.info(f"Analyzing video: {video_info['title']}")

        try:
            agent = self.create_analysis_agent(video_info, transcript)

            # Attach LLM to agent
            llm = await agent.attach_llm(OpenAIAugmentedLLM)

            # Generate analysis using the agent
            result = await llm.generate_str(
                message="ìœ„ ì§€ì‹œì‚¬í•­ì— ë”°ë¼ ì˜ìƒì„ ë¶„ì„í•˜ê³  ì—­ë°œìƒ íˆ¬ìž ì „ëžµì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
                request_params=RequestParams(
                    model="gpt-4.1",
                    maxTokens=16000,
                    max_iterations=3,
                    parallel_tool_calls=False,
                    use_history=True
                )
            )

            logger.info("Analysis completed successfully")
            return result

        except Exception as e:
            logger.error(f"Error during analysis: {e}", exc_info=True)
            return f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def cleanup_temp_files(self):
        """Remove temporary audio files"""
        if AUDIO_FILE.exists():
            try:
                AUDIO_FILE.unlink()
                logger.info("Cleaned up temporary audio file")
            except Exception as e:
                logger.warning(f"Failed to clean up audio file: {e}")

    async def process_new_video(self, video_info: Dict) -> Optional[str]:
        """
        Process a new video: extract audio, transcribe, analyze

        Args:
            video_info: Video metadata dictionary

        Returns:
            Analysis result text, or None on failure
        """
        logger.info(f"Processing new video: {video_info['title']}")

        try:
            # Step 1: Extract audio
            audio_file = self.extract_audio(video_info['link'])
            if not audio_file:
                return None

            # Step 2: Transcribe audio
            transcript = self.transcribe_audio(audio_file)
            if not transcript:
                return None

            # Save transcript for debugging
            transcript_file = EVENTS_DIR / f"transcript_{video_info['id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(transcript_file, 'w', encoding='utf-8') as f:
                f.write(f"Video: {video_info['title']}\n")
                f.write(f"URL: {video_info['link']}\n")
                f.write(f"Published: {video_info['published']}\n")
                f.write(f"\n{'='*80}\n\n")
                f.write(transcript)
            logger.info(f"Transcript saved to: {transcript_file}")

            # Step 3: Analyze content
            analysis = await self.analyze_video(video_info, transcript)

            # Save analysis result
            analysis_file = EVENTS_DIR / f"analysis_{video_info['id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                f.write(analysis)
            logger.info(f"Analysis saved to: {analysis_file}")

            return analysis

        except Exception as e:
            logger.error(f"Error processing video: {e}", exc_info=True)
            return None

        finally:
            # Always cleanup temporary files
            self.cleanup_temp_files()

    async def run(self):
        """Main execution workflow"""
        logger.info("="*80)
        logger.info("YouTube Event Fund Crawler - Starting")
        logger.info("="*80)

        try:
            # Step 1: Fetch latest videos from RSS
            current_videos = self.fetch_latest_videos()
            if not current_videos:
                logger.warning("No videos found in RSS feed")
                return

            # Step 2: Load previous video history
            previous_videos = self.load_previous_videos()

            # Step 3: Find new videos
            new_videos = self.find_new_videos(current_videos, previous_videos)

            if not new_videos:
                logger.info("No new videos found")
                return

            # Step 4: Process each new video
            for video in new_videos:
                logger.info("\n" + "="*80)
                logger.info(f"Processing: {video['title']}")
                logger.info("="*80)

                analysis = await self.process_new_video(video)

                if analysis:
                    # Print analysis to console
                    print("\n" + "="*80)
                    print("ANALYSIS RESULT")
                    print("="*80)
                    print(analysis)
                    print("="*80 + "\n")
                else:
                    logger.warning(f"Failed to analyze video: {video['title']}")

            # Step 5: Save updated video history
            self.save_video_history(current_videos)

            logger.info("="*80)
            logger.info("YouTube Event Fund Crawler - Completed")
            logger.info("="*80)

        except Exception as e:
            logger.error(f"Fatal error in main workflow: {e}", exc_info=True)
            raise


async def main():
    """Entry point"""
    try:
        crawler = YouTubeEventFundCrawler()
        await crawler.run()

    except KeyboardInterrupt:
        logger.info("Interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
